---
layout: post
title: A fun application of simulated annealing
date: 2024-02-15 18:01:35 +0000
categories: jekyll update
usemathjax: true
---
Sometimes a knowledge of computer science or programming ability can turn out to be helpful for physicists. For example, some problems, such as complex systems of differential equations, aren't analytically solvable (at least practically) and so a discrete-time approximation is a good route to take. What's rarer, though, is a physics problem where the challenge on the computer science side isn't just implementation but actual computer science, for example making the best choice of algorithm for a given situation -- even for optimisation problems, a very basic gradient descent setup is typically enough to take care of anything. So it was *not* for this particular problem, where I had to drag in the awesomely named technique of simulated annealing in the process of solving a purely physical problem.

![Emission spectrum of hydrogen atom.](/blog/assets/2024-02-15-welcome-to-jekyll/spectrum-H-real.png)

Above is the emission/absorption spectrum of hydrogen -- the wavelengths of light emitted by a sample of hydrogen when energy is provided to it, or equivalently the wavelengths that would be missing from light shone through such a sample. In truth this is a bit deceptive, since, if you were to actually observe the spectrum through a diffraction grating (an instrument to split light into its component wavelengths), there would be no continuum in the background -- only a black bar punctuated by three vertical lines. Nonetheless, this depiction is helpful for identifying the positions of these colours on the spectrum.

Some readers may already know in great detail why this happens. For those who might not, a brief explanation. It was Bohr and Rutherford in 1913 who first proposed that the atom consisted of a nucleus orbited by discrete shells of electrons, each with an associated amount of energy. An electron transitioning between two of these energy levels will emit a photon with the corresponding change in energy, and Planck had already shown that $E = hf$; that is, that the energy of a photon is proportional to its frequency, with a constant of proportionality of Planck's constant: $6.63 \cdot 10^{-34} \mathrm{Js}$. Since $v = f\lambda$ and $v$ is constant, $\lambda$ in turn is dependent on the energy of the photon. But since there are only finitely many energy levels, each with discrete energies, there are only finitely many possible energy changes, and thus only finitely many possible wavelengths for the emitted photon.

Now let's say we have a set of energy levels within an atom $\\{E_0, E_1, E_2, \cdots\\}$, and we want to use this to find the emission spectrum of the atom. Firstly, we should find all the differences between the energy levels, as these will correspond to the energies of the emitted photons. Then we can get the relevant frequencies and in turn the wavelengths. It's not too bad to put together some code to do all this and spit out a spectrum from the atomic energy levels. Note `E` is the conversion factor from electronvolts to joules. I've also omitted the implementation of `wavelength_to_rgb` because it's not really relevant and is also extremely long-winded.

```python
from itertools import product

import matplotlib.pyplot as plt

H = 6.63e-34
E = 1.602e-19
C = 3e8

with open("levels.txt", "r") as file:
    levels = [float(level) for level in file.readlines()]

# apply equations as discussed earlier
emissions = [a[0] - a[1] for a in product(levels, repeat=2) if a[0] > a[1]]
emissions = [H * C / (emission * E) for emission in emissions]
emissions = [emission for emission in emissions if 4e-7 < emission < 8e-7]

# display non-visible wavelengths as white
emission_colours = [[1, 1, 1] if all([i < 0.001 for i in wavelength_to_rgb(10e8 * emission)]) else wavelength_to_rgb(10e8 * emission) for emission in emissions]

fig, ax = plt.subplots()
ax.set_facecolor("#000000")
fig.set_facecolor("#000000")
fig.set_size_inches(15, 3)

plt.vlines(emissions, 0, 1, emission_colours)
plt.savefig("spectrum.png")
plt.show()
```

![Emission spectrum of mercury atom.](/blog/assets/2024-02-15-welcome-to-jekyll/spectrum-Hg-real.png)
![Emission spectrum of mercury atom, as generated by code.](/blog/assets/2024-02-15-welcome-to-jekyll/spectrum-Hg-generated.png)

Above is the actual spectrum of mercury, and the spectrum as generated by our code -- I've cropped and resized the computationally generated one slightly to emphasize the parallels between it and the real one. As can be seen, it's a pretty good match, with the only actually misplaced line being the green one. There are many dimmer lines on the real spectrum that we don't have, but we can blame this on the data and move on.

What's much more interesting, much more challenging and much more useful is to go the other way. Given an emission spectrum like the one above, can we reverse engineer what the energy levels must be in the original atom? To the best of my knowledge, this would be how the values of these energy levels were actually found, probably long before we had computers to take care of it for us, so it can't be that bad, right?

Turning the wavelengths into frequencies and the frequencies into energy differences is the easy part. The hard part, however, is going from energy differences to actual energy levels -- given some set of differences $\\{d_0, d_1, d_2, \cdots\\}$, how can we find the smallest possible set of values $\\{E_0, E_1, E_2, \cdots\\}$ such that, for every $d_i$, there exist $j$ and $k$ such that $d_i = E_j - E_k$ (or how can we make them as close as possible, since these will all be floating point values and never precisely equal)? Now this is an interesting problem.

My first approach in a situation like this is to write an extremely unoptimized gradient descent program -- essentially, a program that moves downwards along a hypersurface that is the solution space to try and find a minimal cost. If you don't know what that is, 3blue1brown has a very good video: [Gradient descent, how neural networks learn \| Chapter 2, Deep learning](https://www.youtube.com/watch?v=IHZwWFHWa-w). I won't put the code here, because it doesn't work, but the idea was to generate a series of random $E$ values and calculate the cost -- then, add some small number like 0.001 to each of them individually and calculate the change in each case (and divide by that small number) -- in other words, a discrete derivative. This process is very slow, and for any meaningful applications you should really try and mathematically find the derivative of your cost function, but better to have something unoptimized and functional than nothing at all, or at least that's what I told myself while subjecting my CPU to computational torture. Anyway, then take away each $E$ value's cost derivative from itself, and repeat until you have an optimal solution. In theory.

![A plot of an objective function with two minima - a series of dots along the plot, starting from the left and ending in the higher, local minimum, demonstrate the tendency of gradient descent to find local minima.](/blog/assets/2024-02-15-welcome-to-jekyll/objective-function.png)

In reality there are of course flaws with gradient descent, and the most notable is its tendency to find local minima -- solutions with a non-optimal cost, but from which moving in any direction would increase the cost even more, resulting in the algorithm being unable to find any actually decent solution. Above is an instance of a function on which gradient descent, demonstrated roughly by the series of dots, could fail and get stuck in the local minimum. While it is true that gradient descent would find the global minimum if it approached from the other direction, in higher dimensions, there are many more potential directions of approach so we can't rely on following the one that allows us to reach the global minimum.

It's difficult to say why gradient descent performed so horribly in this instance particularly -- perhaps it could've just been something to do with me picking starting values particularly terribly, but this seems unlikely since they were randomly generated on every run. I have a hunch that it might be related to the large number of differences produced by comparatively few $E$ values, and, due to the implicit absolute value function when we take the positive difference, the discontinuity of the solution space, which can also prove troublesome for gradient descent. Focusing on the first point, this means that changing one $E$ affects many calculated differences, and so it could be that an improvement in one difference can never be achieved since it comes hand in hand with worsenings in other differences. To be honest, though, it's not obvious why this would be the case, and I can't do any more than speculate. One idea could be to try and plot the gradient descent surface to get a better understanding of it, but unfortunately with only two $E$ values there is only one difference, so the difficulty of the problem only really arises in higher dimensions which we sadly can't plot.

Fortunately, we can turn to simulated annealing, which has a much reduced tendency to get stuck in local minima.

### Simulated annealing

Simulated annealing (SA) is one of the amazing instances where adding randomness to an algorithm actually improves its performance. Unlike with gradient descent or the closely related algorithm of hill-climbing[^1], where we only transition to states with a lower cost, simulated annealing makes it possible to actually move backwards to positions with a higher cost, and thus explore the solution space more widely.

Returning to our example from the previous section, we see how a few steps backwards would allow us to get over the crest and find a much lower minimum. Of course, we do eventually need the algorithm to stop taking steps backwards and settle on a solution, and this is achieved by slowly decreasing the *temperature*, a quantity representative of how likely we are to take steps that increase the cost, with a high temperature resulting in what is essentially a random walk, and a very low temperature reducing to a hill-climbing algorithm. This is precisely analogous to the metallurgic technique of annealing, in which temperature is gradually reduced to give the metal desirable properties[^2].

Let's take a look at some more details to do with (one way) how simulated annealing can choose whether or not to transition to a given candidate. Given a current cost $C_n$, a transition candidate $C_\text{new}$ and and a temperature $T_n$, we accept the candidate if

$$\exp\left(\frac{C_n - C_\text{new}}{T_n}\right) > \text{random}(0, 1)$$

where $\text{random}(0, 1)$ returns a uniformly distributed random real number in the range $[0, 1]$. This is quite a nice all-in-one expression to do everything that we want. Consider the case when $C_\text{new} < C_n$ -- i.e. the candidate has a lower cost. Then $C_n - C_\text{new}$ will be positive, and since $T_n$ is always positive, we are taking $e$ to the power of a positive number, which is always greater than 1, so we will always accept the candidate.

On the other hand, if $C_\text{new} > C_n$, we have something negative in the brackets. If $T_n$ is high, the expression will have a fairly low magnitude, i.e. be fairly close to 0 -- so when exponentiated, the result will be close to 1 and will be likely to be accepted (see the plot of $\exp(x)$ below). In other words, with high temperatures, even worse candidates are fairly likely to be transitioned to. On the other hand, when $T_n$ is very low, the expression in the brackets will be very negative indeed, so the result of exponentiation is very close to 0, and it becomes vanishingly unlikely that we accept it.

![Plot of the exponential function between x = -5 and x = 1](/blog/assets/2024-02-15-welcome-to-jekyll/exponential.png)

Let's take a look at a Python implementation:

```python
def anneal(differences, n):

    # generate starting values - this shouldn't matter too much for SA
    avg = sum(differences) / len(differences)
    params = [n * avg * random() for i in range(n - 1)]

    # initialise the temperature
    temperature = 50
    
    for i in range(1, ITERATIONS):

        # generate a transition candidate by randomly altering the current solution
        # (which we do by adding a number in the range [-0.025, 0.025] to each E)
        new_params = [param + 0.05*(random() - 0.5) for param in params]

        # accept or reject the candidate as described above
        dc = cost(differences, params) - cost(differences, new_params)
        if safe_exp(dc / (temperature / i**2)) > random():
            params = new_params

    return params
```

One thing you might notice is that I'm using a `safe_exp` function as opposed to `exp` out of the box from `math` or `numpy`. The reason for this is that, when `temperature` gets very low, we're potentially putting some huge numbers into the function, and this can lead to issues even in Python - the `safe_exp` implementation essentially just catches overflow errors and returns positive infinity.

I also have a slightly unusual scheduler, the part of the program that decides what $T_n$ should be -- I define $T_n = \frac{T_0}{n^2}$, when it's more typical to decrease it linearly. I just found that this gave better results.

The cost function is defined as follows:

```python
from itertools import product

def calculate_differences(values):
    return [a[0] - a[1] for a in product(values + [0], repeat=2) if a[0] > a[1]]

def cost(differences, values):
    return sum((a[0] - a[1])**2 for a in zip(sorted(differences), sorted(calculate_differences(values))))
```

This was an instance in which the definition of the cost function was non-trivial. Your first instinct might be (at least mine was) to match every calculated difference with the actual difference that differs from it the least, and take the squared difference between these -- but this is problematic since it results in one calculated difference being compared with many different true differences or vice versa, when each should only really be compared with one other. That's why I chose this implementation, where the lists are sorted and then the differences are paired up with one another. However, this wouldn't work if we were missing some differences. For example, a set of 5 energy levels produces $5 \choose 2$ = 10 differences, but we might only have 9 values, in which case this approach would fail. Then there would be an optimisation problem within an optimisation problem: pair up the calculated differences with the true differences in such a way that no calculated difference is used more than once, and so as to minimize the cost. This is left as an exercise to the reader (and/or my future self).

Anyway, all that remains is to test the code. It's actually surprisingly hard to find good data on the emission spectrum of elements on the Internet -- fortunately, however, Wikipedia has an in-depth article detailing the emission spectrum of hydrogen which cites an even more in-depth paper[^3] (you know it's good when they give the wavelength in air as different from the wavelength in a vacuum). Here's a very small extract of the data provided there (all in nm):

| Starting energy level &#8594;<br>Final energy level &#8595; | 2       | 3       | 4       | 5       | 6       | 7       |
|-------------------------------------------------------------|---------|---------|---------|---------|---------|---------|
| 1                                                           | 121.567 | 102.572 | 97.2537 | 94.9743 | 93.7803 | 93.0748 |
| 2                                                           |         | 656.464 | 486.270 | 434.169 | 410.290 | 397.120 |
| 3                                                           |         |         | 1875.62 | 1282.17 | 1094.11 | 1005.22 |
| 4                                                           |         |         |         | 4052.27 | 2625.87 | 2166.12 |
| 5                                                           |         |         |         |         | 7459.83 | 4653.76 |
| 6                                                           |         |         |         |         |         | 12371.9 |

Of course, the program is not provided with the data in this nice tabular form but just a list of wavelengths. Since we were dealing with 6sf, I upgraded my physical constants to all be to 6sf as well. Running 20 anneals, 14/20 settled on the correct solution (energies in electronvolts):

$$E = \{0, 10.1988, 12.0875, 12.7485, 13.0545, 13.2207, 13.3209\}$$

This is a bit different from what we normally see since all these energies are relative to 0 (for the optimization process, we force 0 to be one of the energy levels). Typically what we have as 0 here is quoted as -13.60eV, or to 6sf -13.5982eV. Shifting all our energies by this value, we finally have

$$E = \{-13.5982, -3.39941, -1.51071, -0.849715, -0.543715, -0.377514, -0.277315\}$$

and these are all correct. In fact, there is a formula, Rydberg's formula, that states

$$\frac{1}{\lambda} = R\left(\frac{1}{n_1^2} - \frac{1}{n_2^2}\right)$$

where $\lambda$ is the wavelength of the emitted light, $n_1$ and $n_2$ are the energy levels, and $R$ is a constant[^4]. Our calculated energies are relative to ionization, i.e. $n_2 \rightarrow \infty$, so the formula simplifies to

$$\frac{1}{\lambda} = \frac{R}{n_1^2}$$

But since $E = hf$, $f = \frac{E}{h}$ -- and since $v = f\lambda$, $\lambda = \frac{v}{f} \Rightarrow \frac{1}{\lambda} = \frac{f}{v} = \frac{E}{hv}$. Therefore,

$$E = \frac{Rhv}{n_1^2} = Rhv \cdot \frac{1}{n_1^2}$$

Now this wouldn't be Physics without a bit of graph-plotting, so let's plot $E$ (technically $-E$) against $\frac{1}{n_1^2}$ and add a line of best fit:

![Plot of 7 points and a line of best fit showing a very strong correlation. It is labelled y = 13.5984x - 0.0002 and R^2 = 1 (6s.f.)](/blog/assets/2024-02-15-welcome-to-jekyll/rydberg-plot.png)

Now $Rhv = 13.5984 \text{eV} = 13.5984 * 1.60218 \cdot 10^{-19} = 2.17870 \cdot 10^{-18} \text{J}$. So

$$
\begin{align*}
R &= \frac{2.17870 \cdot 10^{-18}}{6.62607 \cdot 10^{-34} * 2.99792 \cdot 10^8} \\
&= \boxed{1.09678 \cdot 10^7 \mathrm{m^{-1}}}
\end{align*}$$

and this is equal, to the full 6 significant figures, to the constant's accepted value.

[^1]: Hill-climbing is a similar algorithm to gradient descent, where we never actually calculate the gradient/derivative of the solution space, but instead simply generate random candidate solutions which are adjacent to the current solution, and transition to those if the cost is lower (so in fact it does include randomness to a greater degree than gradient descent). It is more closely related to simulated annealing since SA also adopts the technique of generating random transition candidates.

[^2]: By increasing the temperature of the metal initially, the atoms/ions gain more energy and are able to migrate, reducing dislocations, which are crystal defects that reduce the ductility of the metal. By gradually cooling the metal from there, the atoms/ions settle in a state with fewer dislocations than would be achieved by rapid cooling.

[^3]: [Accurate Atomic Transition Probabilities for Hydrogen, Helium, and Lithium](https://zenodo.org/records/1232309)

[^4]: Technically I should be writing $R_H$, the Rydberg constant for hydrogen, which is notably not the same thing as $R_\infty$.